{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e9bb8c",
   "metadata": {},
   "source": [
    "# Bitwav Tokenizer Demo\n",
    "\n",
    "This notebook demonstrates how to use the Bitwav tokenizer library for:\n",
    "\n",
    "- Speech feature extraction (content tokens and global embeddings)\n",
    "- Speech resynthesis from features\n",
    "- Voice conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073b607",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeeb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9ef90",
   "metadata": {},
   "source": [
    "## 2. Load Models and Audio Data\n",
    "\n",
    "Load the Bitwav model and vocoder, then find example audio files for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitwav import BitwavModel, load_audio, load_vocoder, vocode\n",
    "\n",
    "# Load Bitwav model\n",
    "model = BitwavModel.from_pretrained(\"bitwav/bitwav-25hz-clean\")  # or \"bitwav/bitwav-12.5hz\"\n",
    "\n",
    "model = model.eval().cuda()\n",
    "print(\"Bitwav model loaded\")\n",
    "\n",
    "# Load vocoder\n",
    "vocoder = load_vocoder(model.config.vocoder_name).cuda()\n",
    "print(\"Vocoder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# Find example audio files. Please change the path to your local LibriTTS dataset, or use your own audio files.\n",
    "audio_paths = glob(\"/path/to/LibriTTS/test-clean/**/*.wav\", recursive=True)\n",
    "random.shuffle(audio_paths)\n",
    "example_audio = audio_paths[0]\n",
    "reference_audio_path = audio_paths[1]\n",
    "print(\"Using audio files:\")\n",
    "print(f\"  Source: {example_audio}\")\n",
    "print(f\"  Reference: {reference_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio\n",
    "sample_rate = model.config.sample_rate\n",
    "waveform = load_audio(example_audio, sample_rate=sample_rate)\n",
    "\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Source audio shape: {waveform.shape}\")\n",
    "ipd.display(ipd.Audio(waveform, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc0878",
   "metadata": {},
   "source": [
    "## 3. Extract Speech Features\n",
    "\n",
    "Extract content tokens and global embeddings from the audio using the Bitwav model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the source audio\n",
    "features = model.encode(waveform.cuda())\n",
    "\n",
    "print(\"Extracted features:\")\n",
    "for key, value in features.__dict__.items():\n",
    "    print(f\"  {key}: {value.shape} {value.dtype}\")\n",
    "\n",
    "# Features contain:\n",
    "# - content_token_indices: Discrete tokens representing linguistic content (seq_len,)\n",
    "# - content_embedding: Continuous version of the content tokens (seq_len, dim_content)\n",
    "# - global_embedding: Utterance-level global embedding (dim_global,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2940d",
   "metadata": {},
   "source": [
    "## 4. Speech Resynthesis from Features\n",
    "\n",
    "Resynthesize speech from the extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd403ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesize audio from extracted features\n",
    "mel_spectrogram = model.decode(\n",
    "    content_token_indices=features.content_token_indices, global_embedding=features.global_embedding\n",
    ")\n",
    "resynthesized_waveform = vocode(vocoder, mel_spectrogram.unsqueeze(0))  # (1, samples)\n",
    "\n",
    "# The target audio length is estimated from the content token length, which may differ from the original audio length\n",
    "print(f\"Resynthesized waveform shape: {resynthesized_waveform.shape}\")\n",
    "ipd.display(ipd.Audio(resynthesized_waveform.cpu(), rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2b765",
   "metadata": {},
   "source": [
    "## 5. Voice Conversion\n",
    "\n",
    "Convert the voice of one speaker to sound like another while preserving the linguistic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference audio for voice conversion\n",
    "reference_audio = load_audio(reference_audio_path, sample_rate=sample_rate)\n",
    "print(f\"Reference audio shape: {reference_audio.shape}\")\n",
    "ipd.display(ipd.Audio(reference_audio, rate=sample_rate))\n",
    "\n",
    "# Extract source content features and reference global features\n",
    "source_features = model.encode(waveform.cuda(), return_content=True, return_global=False)\n",
    "reference_features = model.encode(reference_audio.cuda(), return_content=False, return_global=True)\n",
    "\n",
    "# Perform voice conversion\n",
    "converted_mel_spectrogram = model.decode(\n",
    "    content_embedding=source_features.content_embedding,\n",
    "    global_embedding=reference_features.global_embedding,\n",
    "    target_audio_length=waveform.size(0),\n",
    ")\n",
    "\n",
    "# Or use the convenience method:\n",
    "# converted_mel_spectrogram = model.voice_conversion(source_waveform=waveform.cuda(), reference_waveform=reference_audio.cuda())\n",
    "\n",
    "converted_waveform = vocode(vocoder, converted_mel_spectrogram.unsqueeze(0))  # (1, samples)\n",
    "\n",
    "print(f\"Converted waveform shape: {converted_waveform.shape}\")\n",
    "ipd.display(ipd.Audio(converted_waveform.cpu(), rate=sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitwav (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
